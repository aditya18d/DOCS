# Step 1: Import Required Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Step 2: Load and Prepare Data
df = pd.read_csv('/content/studentData.txt', header=None)
df.columns = ['Exam1', 'Exam2', 'Admitted']
X = df[['Exam1', 'Exam2']].values
y = df['Admitted'].values.reshape(-1, 1)
m = y.shape[0]
X = np.concatenate([np.ones((m, 1)), X], axis=1)  # Add bias term

# Step 3: Visualize Data
admitted = y.flatten() == 1
not_admitted = y.flatten() == 0

plt.plot(X[admitted, 1], X[admitted, 2], 'bx', label='Admitted')
plt.plot(X[not_admitted, 1], X[not_admitted, 2], 'ro', label='Not Admitted')
plt.xlabel('Exam 1 Score')
plt.ylabel('Exam 2 Score')
plt.title('Admission Plot')
plt.legend()
plt.show()

# Step 4: Define Sigmoid, Cost, and Gradient Descent
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_cost(X, y, theta):
    h = sigmoid(X @ theta)
    e = 1e-7
    return (-1/m) * np.sum(y * np.log(h + e) + (1 - y) * np.log(1 - h + e))

def gradient_descent(X, y, alpha, iterations):
    theta = np.zeros((X.shape[1], 1))
    cost_history = []
    for i in range(iterations):
        h = sigmoid(X @ theta)
        theta -= (alpha / m) * X.T @ (h - y)
        if i % 10000 == 0:
            print(f"Iteration {i}, Cost: {compute_cost(X, y, theta)}")
        cost_history.append(compute_cost(X, y, theta))
    return theta, cost_history

# Step 5: Train the Model
theta, cost_history = gradient_descent(X, y, alpha=0.001, iterations=200000)

# Step 6: Plot Cost Over Iterations
plt.plot(cost_history)
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.title('Cost Reduction Over Time')
plt.grid(True)
plt.show()